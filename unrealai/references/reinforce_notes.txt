gymnasium- colleciton of environments, typically gamelike with rules

basic concepts: Environments, Agents, Obsevations, Action, Reward

Environments have well defined rules


The basic process for using Gymnasium to train a Reinforcement Learning model is as follows:
1. Define the environment you want to work with.
2. Create an instance of the environment.
3. Define the agent's policy (i.e., how it decides which action to take).
4. Interact with the environment, taking actions and receiving rewards.
5. Update the agent's policy based on the rewards it receives.
6. Repeat steps 4 and 5 until the agent's performance is satisfactory.

#1. Observation and Action Space: An observation space is the set of possible states that an agent can observe in the environment. An action space is the set of possible actions that an agent can take in the environment.

#2. Episode: An episode is a complete run-through of an environment, starting from the initial state and continuing until a terminal state is reached. Each episode is composed of a sequence of states, actions, and rewards.

#3. Wrapper: A wrapper is a tool in Gymnasium that allows you to modify an environment's behavior without changing its code. Wrappers can be used to add features such as time limits, reward shaping, and action masking.

#4. Benchmark Gymnasium provides a set of benchmark environments, which are standardized tasks that can be used to evaluate and compare reinforcement learning algorithms. These benchmarks include classic control tasks, Atari games, and robotics tasks.


#deep q uses a combo of modelless qlearning and a neural network

1. Replay Memory is a technique used in reinforcement learning to store and manage the experiences
of an agent during training. The idea is to store the agent's experiences as a sequence of
(state, action, reward, next_state) tuples, which are collected as the agent interacts with
the environment. During training, these experiences are used to update the agent's policy and
value function.
2. The Replay Memory allows the agent to learn from past experiences by randomly sampling a batch
of experiences from the memory buffer, rather than just learning from the most recent experience.
This helps to reduce the correlation between subsequent experiences, which can improve the
stability and convergence of the learning algorithm. In addition, by storing experiences in a buffer, the agent can re-use past experiences to update its policy and value function multiple times, which can further improve learning efficiency.
3. The Replay Memory is typically implemented as a fixed-size buffer or queue that stores the most
recent experiences. When the buffer is full, new experiences overwrite the oldest experiences in the
buffer. During training, a batch of experiences is randomly sampled from the buffer and used to
update the agent's policy and value function. This process is repeated iteratively until the agent
converges to an optimal policy.
